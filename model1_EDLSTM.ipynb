{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Model1_Encoder_Decoder_LSTM.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMldi9Jo6R9GQCQodu38jKC"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qDKMog461J_l","executionInfo":{"status":"ok","timestamp":1607938388303,"user_tz":360,"elapsed":653,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}},"outputId":"0dedc7d4-3213-4a91-be23-9de5e19e9431"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":150,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u631zTBG1MFo","executionInfo":{"status":"ok","timestamp":1607938388788,"user_tz":360,"elapsed":1114,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}},"outputId":"e6d2380e-4405-4a93-824b-a1737c0bbba6"},"source":["%cd '/content/drive/My Drive/ModelSharing'"],"execution_count":151,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/ModelSharing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gPPqYh3U1VL4","executionInfo":{"status":"ok","timestamp":1607938388789,"user_tz":360,"elapsed":1099,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["import numpy as np\r\n","import pandas as pd\r\n","\r\n","from tensorflow.keras.models import *\r\n","from tensorflow.keras.layers import *\r\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\r\n","from tensorflow.keras.optimizers import *\r\n","import tensorflow.keras.backend as K\r\n","\r\n","from sklearn.preprocessing import MinMaxScaler\r\n","from sklearn.model_selection import train_test_split"],"execution_count":152,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGyMOPlH1ihI","executionInfo":{"status":"ok","timestamp":1607938388790,"user_tz":360,"elapsed":1089,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["def series_to_supervised(data, n_in=1, n_out=1, dropnan=False):\r\n","  n_vars = 1 if type(data) is list else data.shape[1]\r\n","  df = pd.DataFrame(data)\r\n","  cols, names = list(), list()\r\n","  # input sequence (t-n, ... t-1)\r\n","  for i in range(n_in, 0, -1):\r\n","    cols.append(df.shift(i))\r\n","    names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\r\n","  # forecast sequence (t, t+1, ... t+n)\r\n","  for i in range(0, n_out):\r\n","    cols.append(df.shift(-i))\r\n","    if i == 0:\r\n","      names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\r\n","    else:\r\n","      names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\r\n","  # put it all together\r\n","  agg = pd.concat(cols, axis=1)\r\n","  agg.columns = names\r\n","  # drop rows with NaN values\r\n","  if dropnan:\r\n","    agg.dropna(inplace=True)\r\n","  return pd.DataFrame(agg.astype('float32'))"],"execution_count":153,"outputs":[]},{"cell_type":"code","metadata":{"id":"c46oMGAs_Ile","executionInfo":{"status":"ok","timestamp":1607938388791,"user_tz":360,"elapsed":1082,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["def train_valid_test_split(data, hours_of_history, hours_to_predict, parameters_included):\r\n","  data_train_valid = data.iloc[:52608,:] # the first 6 years for training/validation\r\n","  data_test = data.iloc[52608:,:] # the last 1 years for test evaluation\r\n","  data_train_valid.dropna(inplace=True)\r\n","  data_test.dropna(inplace=True)\r\n","\r\n","  data_valid, data_train = train_test_split(data_train_valid, test_size=0.4, shuffle= False) # the last 60% data in the first 6 years used for training and the first 40% used for validation.\r\n","\r\n","  return data_train.values, data_valid.values, data_test.values"],"execution_count":154,"outputs":[]},{"cell_type":"code","metadata":{"id":"kFW20INaHjoO","executionInfo":{"status":"ok","timestamp":1607938388793,"user_tz":360,"elapsed":1074,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["def prepare_data(station_id, hours_of_history, hours_to_predict, parameters_included):\r\n","\r\n","  data = pd.read_csv('./data/'+str(station_id)+'_data.csv').iloc[:,1:]\r\n","\r\n","  # simple min-max scaling. Other pretreatments such as normalization also work.\r\n","  scaler = MinMaxScaler()\r\n","  scaler.fit(data.iloc[:52608,:]) # min-max scaling without the test dataset.\r\n","  q_max = np.max(data.iloc[:52608,2]) # manually check the maximum and minimum discharge\r\n","  q_min = np.min(data.iloc[:52608,2])\r\n","  data_scaled = scaler.transform(data)\r\n","\r\n","  # data split\r\n","  data_sequence = series_to_supervised(data_scaled, hours_of_history, hours_to_predict)\r\n","  data_train, data_valid, data_test = train_valid_test_split(data_sequence, hours_of_history, hours_to_predict, parameters_included)\r\n","\r\n","  # Split data into 2 parts for encoder (history) and decoder(future).\r\n","  train_x_rainfall = data_train[:,0::3].reshape(-1, hours_of_history+hours_to_predict, 1)\r\n","  train_discharge = data_train[:,2::3].reshape(-1, hours_of_history+hours_to_predict, 1)\r\n","  train_x_discharge = train_discharge[:,:hours_of_history,:]\r\n","  train_y = train_discharge[:,hours_of_history:,:]\r\n","  train_x_et = data_train[:,3*hours_of_history+1].reshape(-1, 1) # the current hour et\r\n","\r\n","  valid_x_rainfall = data_valid[:,0::3].reshape(-1, hours_of_history+hours_to_predict, 1)\r\n","  valid_discharge = data_valid[:,2::3].reshape(-1, hours_of_history+hours_to_predict, 1)\r\n","  valid_x_discharge = valid_discharge[:,:hours_of_history,:]\r\n","  valid_y = valid_discharge[:,hours_of_history:,:]\r\n","  valid_x_et = data_valid[:,3*hours_of_history+1].reshape(-1, 1) # the current hour et\r\n","\r\n","  test_x_rainfall = data_test[:,0::3].reshape(-1, hours_of_history+hours_to_predict, 1)\r\n","  test_discharge = data_test[:,2::3].reshape(-1, hours_of_history+hours_to_predict, 1)\r\n","  test_x_discharge = test_discharge[:,:hours_of_history,:]\r\n","  test_y = test_discharge[:,hours_of_history:,:]\r\n","  test_x_et = data_test[:,3*hours_of_history+1].reshape(-1, 1) # the current hour et\r\n","\r\n","  return [train_x_et, train_x_discharge, train_x_rainfall], train_y, [valid_x_et, valid_x_discharge, valid_x_rainfall], valid_y, [test_x_et, test_x_discharge, test_x_rainfall], test_y, q_max, q_min"],"execution_count":155,"outputs":[]},{"cell_type":"code","metadata":{"id":"_w-hAdAcFqNl","executionInfo":{"status":"ok","timestamp":1607938388795,"user_tz":360,"elapsed":1064,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["# define custome loss function (you can use the simple 'mse' as well)\r\n","def nseloss(y_true, y_pred):\r\n","  return K.sum((y_pred-y_true)**2)/K.sum((y_true-K.mean(y_true))**2)"],"execution_count":156,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vy6tFQRBRuUD","executionInfo":{"status":"ok","timestamp":1607938388798,"user_tz":360,"elapsed":1058,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["def EDLSTM(hours_of_history, hours_to_predict, parameters_included):\r\n","\r\n","  # design network\r\n","  # input of runoff observation, LSTM encoder\r\n","  input_1 = Input(shape=(hours_of_history, 1), name='LSTM1_input') # shape should be 72*1 for runoff observation\r\n","  LSTM1 = LSTM(256, return_sequences=False)(input_1)\r\n","\r\n","  # input of rainfall observation and forecast, LSTM encoder\r\n","  input_2 = Input(shape=((hours_of_history+hours_to_predict), 1), name='LSTM2_input') # shape should be (72+24)*n=96*n, for rainfall observation (72) and predictions (24) for rainfall and additional stations (if there is no upstream station, n=1)\r\n","  LSTM2 = LSTM(256, return_sequences=False)(input_2)\r\n","\r\n","  # input of other non-timeseries data, such as daily or monthly data.\r\n","  input_phys = Input(shape=(1,), name='phys_input') # one single value of ET. shape = 1.\r\n","\r\n","  # connect all data\r\n","  x = concatenate([input_phys, LSTM1, LSTM2]) # Get state vector.\r\n","  x = RepeatVector(24)(x) # 24 is the output time dimension\r\n","\r\n","  # LSTM decoder\r\n","  x = LSTM(512, return_sequences=True)(x)\r\n","\r\n","  # define fully-connected dense layers\r\n","  dim_dense=[512, 256, 256, 128, 64]\r\n","\r\n","  # final fully-connected dense layer for final result\r\n","  for dim in dim_dense:\r\n","    x = TimeDistributed(Dense(dim, activation='relu'))(x)\r\n","    x = TimeDistributed(Dropout(0.2))(x) # Some dropout for dense layers. Some paper mentioned that it is not recommend to have dropout between the RNN/LSTM/GRU layers. Thus, I only apply dropout in the dense layer.\r\n","  main_out = TimeDistributed(Dense(1, activation='relu'))(x) # here relu provides the final output non-negative, which is corrosponding to my min-max pre-prossing.\r\n","  main_out = Flatten()(main_out)\r\n","\r\n","  model = Model(inputs=[input_phys, input_1, input_2], outputs=main_out)\r\n","  \r\n","  return model"],"execution_count":157,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvTU6Ob4cc8A","executionInfo":{"status":"ok","timestamp":1607938388799,"user_tz":360,"elapsed":1047,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["# identify KGE, NSE for evaluation\r\n","def nse(y_true, y_pred):\r\n","  return 1-np.sum((y_pred-y_true)**2)/np.sum((y_true-np.mean(y_true))**2)\r\n","  \r\n","def kge(y_true, y_pred):\r\n","  kge_r = np.corrcoef(y_true,y_pred)[1][0]\r\n","  kge_a = np.std(y_pred)/np.std(y_true)\r\n","  kge_b = np.mean(y_pred)/np.mean(y_true)\r\n","  return 1-np.sqrt((kge_r-1)**2+(kge_a-1)**2+(kge_b-1)**2)\r\n"],"execution_count":158,"outputs":[]},{"cell_type":"code","metadata":{"id":"MptO3dI7Hucj","executionInfo":{"status":"ok","timestamp":1607938388801,"user_tz":360,"elapsed":1039,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":["def main():\r\n","  \r\n","  # parameters\r\n","  station_id = 521\r\n","  hours_to_predict = 24\r\n","  hours_of_history = 72\r\n","  parameters_included = 3\r\n","\r\n","  batch_size = 64\r\n","  lr = 0.0001\r\n","  epochs = 300\r\n","  test_name = './'+str(station_id)+'_model1_'\r\n","\r\n","  # load data\r\n","  x_train, y_train, x_valid, y_valid, x_test, y_test, q_max, q_min = prepare_data(station_id, hours_of_history, hours_to_predict, parameters_included)\r\n","  model1 = EDLSTM(hours_of_history, hours_to_predict, parameters_included)\r\n","\r\n","  # compile settings\r\n","  reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=15, cooldown=30, min_lr=1e-8)\r\n","  earlystoping = EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1, mode='auto')\r\n","  checkpoint = ModelCheckpoint(test_name+'model.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\r\n","  optimizer = RMSprop(lr=lr)\r\n","  \r\n","  model1.compile(optimizer=optimizer, loss=nseloss) # I used the build-in RMSprop, loss function is 1-NSE. You can use 'mse', 'mae' as well.\r\n","\r\n","  # train model\r\n","  history = model1.fit(x_train, y_train, epochs=epochs, batch_size=batch_size,\r\n","              validation_data=(x_valid, y_valid), callbacks=[reduce_lr, earlystoping, checkpoint], verbose=1)\r\n","\r\n","  # save training loss\r\n","  loss_train = history.history['loss']\r\n","  loss_valid = history.history['val_loss']\r\n","  loss_train = pd.DataFrame({'TrainLoss':loss_train})\r\n","  loss_valid = pd.DataFrame({'TestLoss':loss_valid})\r\n","  LossEpoches = pd.concat([loss_train, loss_valid], axis=1)\r\n","  LossEpoches.to_csv(test_name+'loss.csv', index = True)\r\n","\r\n","  # Final Test Review\r\n","  model1.load_weights(test_name+'model.h5')\r\n","\r\n","  y_model_scaled = model1.predict(x_test)\r\n","  y_model = y_model_scaled*(q_max-q_min)+q_min\r\n","  y_test = y_test*(q_max-q_min)+q_min\r\n","\r\n","  # hourly evaluation\r\n","  NSEs=[]\r\n","  KGEs=[]\r\n","  for x in range(0, 24):\r\n","    y_pred = y_model[:,x]\r\n","    y_True = y_test[:,x]\r\n","    NSEs.append(nse(y_True[:,0],y_pred))\r\n","    KGEs.append(kge(y_True[:,0],y_pred))  \r\n","    \r\n","  NSEs=pd.DataFrame(NSEs)\r\n","  NSEs.columns = ['NSE_Test']\r\n","  KGEs=pd.DataFrame(KGEs)\r\n","  KGEs.columns = ['KGE_Test']\r\n","    \r\n","  eva = pd.concat([NSEs, KGEs], axis=1)\r\n","  eva.to_csv(test_name+'eva.csv', index = True)\r\n"," "],"execution_count":159,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o03f8xK7ao9g","executionInfo":{"status":"ok","timestamp":1607938836656,"user_tz":360,"elapsed":444533,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}},"outputId":"8425a0d2-1c7b-433f-839f-26271924be2a"},"source":["if __name__ == \"__main__\":\r\n","  main()\r\n"],"execution_count":160,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \"\"\"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.4441\n","Epoch 00001: val_loss improved from inf to 2470.20923, saving model to ./521_model1_model.h5\n","288/288 [==============================] - 13s 47ms/step - loss: 0.4436 - val_loss: 2470.2092\n","Epoch 2/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.2392\n","Epoch 00002: val_loss improved from 2470.20923 to 187.00623, saving model to ./521_model1_model.h5\n","288/288 [==============================] - 12s 41ms/step - loss: 0.2391 - val_loss: 187.0062\n","Epoch 3/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.2062\n","Epoch 00003: val_loss improved from 187.00623 to 181.50035, saving model to ./521_model1_model.h5\n","288/288 [==============================] - 12s 43ms/step - loss: 0.2059 - val_loss: 181.5004\n","Epoch 4/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1869\n","Epoch 00004: val_loss did not improve from 181.50035\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1869 - val_loss: 4390.8882\n","Epoch 5/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1644\n","Epoch 00005: val_loss did not improve from 181.50035\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1642 - val_loss: 577.6963\n","Epoch 6/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1587\n","Epoch 00006: val_loss did not improve from 181.50035\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1587 - val_loss: 351.6163\n","Epoch 7/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1432\n","Epoch 00007: val_loss improved from 181.50035 to 153.86659, saving model to ./521_model1_model.h5\n","288/288 [==============================] - 12s 43ms/step - loss: 0.1433 - val_loss: 153.8666\n","Epoch 8/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1354\n","Epoch 00008: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1352 - val_loss: 446.7386\n","Epoch 9/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1318\n","Epoch 00009: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1315 - val_loss: 2113.2480\n","Epoch 10/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1218\n","Epoch 00010: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1217 - val_loss: 742.7985\n","Epoch 11/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1196\n","Epoch 00011: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1195 - val_loss: 383.6332\n","Epoch 12/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1233\n","Epoch 00012: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1233 - val_loss: 251.3434\n","Epoch 13/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1120\n","Epoch 00013: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1119 - val_loss: 230.2748\n","Epoch 14/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1102\n","Epoch 00014: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1101 - val_loss: 281.5997\n","Epoch 15/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1109\n","Epoch 00015: val_loss did not improve from 153.86659\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1107 - val_loss: 620.1617\n","Epoch 16/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1016\n","Epoch 00016: val_loss improved from 153.86659 to 82.68546, saving model to ./521_model1_model.h5\n","288/288 [==============================] - 12s 43ms/step - loss: 0.1016 - val_loss: 82.6855\n","Epoch 17/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0994\n","Epoch 00017: val_loss improved from 82.68546 to 79.11698, saving model to ./521_model1_model.h5\n","288/288 [==============================] - 12s 43ms/step - loss: 0.0993 - val_loss: 79.1170\n","Epoch 18/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0920\n","Epoch 00018: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0919 - val_loss: 991.9701\n","Epoch 19/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0910\n","Epoch 00019: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0911 - val_loss: 550.6533\n","Epoch 20/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0953\n","Epoch 00020: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0952 - val_loss: 306.4453\n","Epoch 21/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.1042\n","Epoch 00021: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.1041 - val_loss: 529.1093\n","Epoch 22/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0848\n","Epoch 00022: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0850 - val_loss: 3158.7556\n","Epoch 23/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0848\n","Epoch 00023: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0847 - val_loss: 592.3381\n","Epoch 24/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0832\n","Epoch 00024: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0831 - val_loss: 442.6578\n","Epoch 25/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0809\n","Epoch 00025: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0809 - val_loss: 747.2086\n","Epoch 26/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0754\n","Epoch 00026: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0754 - val_loss: 915.9394\n","Epoch 27/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0726\n","Epoch 00027: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0730 - val_loss: 103.9680\n","Epoch 28/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0742\n","Epoch 00028: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0741 - val_loss: 120.4921\n","Epoch 29/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0732\n","Epoch 00029: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0732 - val_loss: 1311.6111\n","Epoch 30/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0707\n","Epoch 00030: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0707 - val_loss: 2302.3806\n","Epoch 31/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0700\n","Epoch 00031: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0699 - val_loss: 540.1257\n","Epoch 32/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0680\n","Epoch 00032: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0679 - val_loss: 84.9262\n","Epoch 33/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0466\n","Epoch 00033: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 41ms/step - loss: 0.0466 - val_loss: 384.0522\n","Epoch 34/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0434\n","Epoch 00034: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0435 - val_loss: 1208.3362\n","Epoch 35/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0427\n","Epoch 00035: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0426 - val_loss: 121.9358\n","Epoch 36/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0414\n","Epoch 00036: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0413 - val_loss: 405.5717\n","Epoch 37/300\n","287/288 [============================>.] - ETA: 0s - loss: 0.0402\n","Epoch 00037: val_loss did not improve from 79.11698\n","288/288 [==============================] - 12s 40ms/step - loss: 0.0401 - val_loss: 481.2160\n","Epoch 00037: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s2fcQj_ET3oG","executionInfo":{"status":"ok","timestamp":1607938836664,"user_tz":360,"elapsed":42,"user":{"displayName":"Zr Xiang","photoUrl":"","userId":"10347344196072452629"}}},"source":[""],"execution_count":160,"outputs":[]}]}